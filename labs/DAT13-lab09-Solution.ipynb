{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# GA Data Science SF 13 (DAT13) - Lab9\n",
      "\n",
      "### Logistic Regression\n",
      "\n",
      "thanks to [Olivier Grisel](https://github.com/ogrisel/parallel_ml_tutorial)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Short Version: Logistic Regression on Titanic Dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import classification_report\n",
      "\n",
      "\n",
      "# import data\n",
      "data = pd.read_csv('titanic-train.csv')\n",
      "#fill na, define features and target numpy arrays\n",
      "numerical_features = data.get(['Fare', 'Pclass', 'Age'])\n",
      "features_array = numerical_features.fillna(numerical_features.dropna().median()).values\n",
      "target = data.Survived.values\n",
      "\n",
      "# train test split\n",
      "features_train, features_test, target_train, target_test = train_test_split(features_array, target, test_size=0.20, random_state=0)\n",
      "\n",
      "\n",
      "# train logistic regression, evaluate on test\n",
      "lr = LogisticRegression(C=1)\n",
      "lr.fit(features_train, target_train)\n",
      "target_predicted = lr.predict(features_test)\n",
      "\n",
      "print(target_test)\n",
      "print(target_predicted)\n",
      "\n",
      "#evaluate accuracy\n",
      "#print(\"\\n\\nLogistic regression of Titanic Dataset on Numerical Features\\n\\n\")\n",
      "\n",
      "#print(classification_report(target_test, target_predicted,\n",
      "#                            target_names=['not survived', 'survived']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0\n",
        " 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 0\n",
        " 0 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0\n",
        " 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1\n",
        " 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0]\n",
        "[0 0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0\n",
        " 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0\n",
        " 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
        " 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
        " 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Example: Logistic Regression On Titanic Dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = pd.read_csv('titanic-train.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.Survived.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "0    0\n",
        "1    1\n",
        "2    1\n",
        "3    1\n",
        "4    0\n",
        "Name: Survived, dtype: int64"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Question: what is our prediction benchmark for model?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "1 - data.Survived.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "0.61616161616161613"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From this the subset of the full passengers list, about 2/3 perished in the event. So if we are to build a predictive model from this data, a baseline model to compare the performance to would be to always predict death. Such a constant model would reach around 62% predictive accuracy (which is higher than predicting at random):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target = data.Survived.values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`sklearn` estimators all work with homegeneous numerical feature descriptors passed as a numpy array. Therefore passing the raw data frame will not work out of the box.\n",
      "\n",
      "Let us start simple and build a first model that only uses readily available numerical features as input, namely `data.Fare`, `data.Pclass` and `data.Age`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "numerical_features = data.get(['Fare', 'Pclass', 'Age'])\n",
      "numerical_features.head(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Fare</th>\n",
        "      <th>Pclass</th>\n",
        "      <th>Age</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>  7.2500</td>\n",
        "      <td> 3</td>\n",
        "      <td> 22</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 71.2833</td>\n",
        "      <td> 1</td>\n",
        "      <td> 38</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>  7.9250</td>\n",
        "      <td> 3</td>\n",
        "      <td> 26</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 53.1000</td>\n",
        "      <td> 1</td>\n",
        "      <td> 35</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>  8.0500</td>\n",
        "      <td> 3</td>\n",
        "      <td> 35</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 3 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "      Fare  Pclass  Age\n",
        "0   7.2500       3   22\n",
        "1  71.2833       1   38\n",
        "2   7.9250       3   26\n",
        "3  53.1000       1   35\n",
        "4   8.0500       3   35\n",
        "\n",
        "[5 rows x 3 columns]"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unfortunately some passengers do not have age information:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "numerical_features.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "Fare      891\n",
        "Pclass    891\n",
        "Age       714\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's use pandas `fillna` method to input the median age for those passengers:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "median_features = numerical_features.dropna().median()\n",
      "median_features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "Fare      15.7417\n",
        "Pclass     2.0000\n",
        "Age       28.0000\n",
        "dtype: float64"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "imputed_features = numerical_features.fillna(median_features)\n",
      "imputed_features.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "Fare      891\n",
        "Pclass    891\n",
        "Age       891\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "imputed_features.head(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Fare</th>\n",
        "      <th>Pclass</th>\n",
        "      <th>Age</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>  7.2500</td>\n",
        "      <td> 3</td>\n",
        "      <td> 22</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 71.2833</td>\n",
        "      <td> 1</td>\n",
        "      <td> 38</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>  7.9250</td>\n",
        "      <td> 3</td>\n",
        "      <td> 26</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 53.1000</td>\n",
        "      <td> 1</td>\n",
        "      <td> 35</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>  8.0500</td>\n",
        "      <td> 3</td>\n",
        "      <td> 35</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 3 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "      Fare  Pclass  Age\n",
        "0   7.2500       3   22\n",
        "1  71.2833       1   38\n",
        "2   7.9250       3   26\n",
        "3  53.1000       1   35\n",
        "4   8.0500       3   35\n",
        "\n",
        "[5 rows x 3 columns]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that the data frame is clean, we can convert it into an homogeneous numpy array of floating point values:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_array = imputed_features.values\n",
      "features_array"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "array([[  7.25  ,   3.    ,  22.    ],\n",
        "       [ 71.2833,   1.    ,  38.    ],\n",
        "       [  7.925 ,   3.    ,  26.    ],\n",
        "       ..., \n",
        "       [ 23.45  ,   3.    ,  28.    ],\n",
        "       [ 30.    ,   1.    ,  26.    ],\n",
        "       [  7.75  ,   3.    ,  32.    ]])"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's take the 80% of the data for training a first model and keep 20% for computing is generalization score:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "features_train, features_test, target_train, target_test = train_test_split(\n",
      "    features_array, target, test_size=0.20, random_state=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "(712, 3)"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "(179, 3)"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "(712,)"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "(179,)"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's start with a simple model from sklearn, namely `LogisticRegression`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "lr = LogisticRegression(C=1)\n",
      "lr.fit(features_train, target_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target_predicted = lr.predict(features_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "accuracy_score(target_test, target_predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "0.73184357541899436"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This first model has around 73% accuracy: this is better than our baselines that always predicts death."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Model evaluation and interpretation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Interpreting linear model weights"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `coef_` attribute of a fitted linear model such as `LogisticRegression` holds the weights of each features:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_names = numerical_features.columns.values\n",
      "feature_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "array(['Fare', 'Pclass', 'Age'], dtype=object)"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr.coef_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "array([[ 0.0043996 , -0.80916725, -0.03348064]])"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.arange(len(feature_names))\n",
      "plt.bar(x, lr.coef_.ravel())\n",
      "_ = plt.xticks(x + 0.5, feature_names, rotation=30)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEJCAYAAABv6GdPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10b65d650>"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case, survival is slightly positively linked with Fare (the higher the fare, the higher the likelyhood the model will predict survival) while passenger from first class and lower ages are predicted to survive more often than older people from the 3rd class.\n",
      "\n",
      "First-class cabins where closer to the lifeboats and children and women reportedly had the priority. Our model seems to capture that historical data. We will see later if the sex of the passenger can be used as an informative predictor to increase the predictive accuracy of the model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Predicting the Probability of Being in Class 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Logistic Regression is a probabilistic models: instead of just predicting a binary outcome (survived or not) given the input features it can also estimates the posterior probability of the outcome given the input features using the `predict_proba` method:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target_predicted_proba = lr.predict_proba(features_test)\n",
      "target_predicted_proba[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "array([[ 0.75263264,  0.24736736],\n",
        "       [ 0.75824771,  0.24175229],\n",
        "       [ 0.58542437,  0.41457563],\n",
        "       [ 0.25224882,  0.74775118],\n",
        "       [ 0.75817844,  0.24182156]])"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By default the decision threshold is 0.5: if we vary the decision threshold from 0 to 1 we could generate a family of binary classifier models that address all the possible trade offs between false positive and false negative prediction errors."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "print(confusion_matrix(target_test, target_predicted))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[98 12]\n",
        " [36 33]]\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another way to quantify the quality of a binary classifier on imbalanced data is to compute the precision, recall and f1-score of a model (at the default fixed decision threshold of 0.5)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import classification_report\n",
      "\n",
      "print(classification_report(target_test, target_predicted,\n",
      "                            target_names=['not survived', 'survived']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "              precision    recall  f1-score   support\n",
        "\n",
        "not survived       0.73      0.89      0.80       110\n",
        "    survived       0.73      0.48      0.58        69\n",
        "\n",
        " avg / total       0.73      0.73      0.72       179\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the Docs:\n",
      "\n",
      "```python\n",
      "class sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None)\u00b6\n",
      "```\n",
      "\n",
      "\n",
      "| Attributes\n",
      "|:-----------|:----------|:----------|\n",
      "| coef_ | array, shape = [n_classes-1, n_features]| Coefficient of the features in the decision function. coef_ is readonly property derived from raw_coef_ that follows the internal memory layout of liblinear.|\n",
      "| intercept_  |array, shape = [n_classes-1] | Intercept (a.k.a. bias) added to the decision function. It is available only when parameter intercept is set to True.|\n",
      "| random_state: int seed, RandomState instance, or None (default) | |The seed of the pseudo random number generator to use when shuffling the data.|\n",
      "\n",
      "\n",
      "\n",
      "| Methods\n",
      "|:-----------|:----------|\n",
      "| decision_function(X) | Predict confidence scores for samples.|\n",
      "| densify() |Convert coefficient matrix to dense array format.|\n",
      "| fit(X, y) |Fit the model according to the given training data.|\n",
      "| fit_transform(X[, y]) |Fit to data, then transform it.|\n",
      "| get_params([deep]) | Get parameters for this estimator.|\n",
      "| predict(X)  | Predict class labels for samples in X.|\n",
      "| predict_log_proba(X) | Log of probability estimates.|\n",
      "| predict_proba(X) | Probability estimates.|\n",
      "| score(X, y) |  Returns the mean accuracy on the given test data and labels.|\n",
      "| set_params(\\*\\*params) | Set the parameters of this estimator.|\n",
      "| sparsify() | Convert coefficient matrix to sparse format.|\n",
      "| transform(X[, threshold]) |Reduce X to its most important features.|"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}