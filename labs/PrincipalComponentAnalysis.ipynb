{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets, random_projection \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is the process of reducing the number of random variables (features) under consideration. The process of dimensionality reduction has a variety of applications and is used throughout the domain of data mining. Here we will explore the concepts behind Prinicpal Component Analysis (PCA), and step through a couple of examples. \n",
    "- The first example is the cannonical PCA example.\n",
    "- We will also examine the handwritten digits dataset, specifically clustering by prinicpal components.\n",
    "- Finally, the class will implement PCA on the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA: Take a teapot, how can I rotate it to give the most information to an observer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"source_data/teapot1.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cannonical PCA example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Make a random dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cov = np.array([[2.9, -2.2], [-2.2, 6.5]])\n",
    "norm_dist = np.random.multivariate_normal([1,2], cov, size=500)  #Makes a random set of data (drawing from a multivariate normal distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax.scatter(norm_dist[:,0], norm_dist[:,1], color='r')\n",
    "ax.axis('equal') # equal scaling on both axis;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### We would now like to analyze the directions in which the data varies most. \n",
    "\n",
    "##### For that, we place the point cloud in the center (0,0) and rotate a line through the data, such that the direction with most variance is parallel to the x-axis. Each succeding component in turn accounts for the highest variance possible that is orthoganal to existing components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, let's perform principal component analysis (PCA) to project the data into fewer dimensions. In PCA, the projection is defined by principal components (eigenvectors), each of which can be viewed as a linear combination of the original features that corresponds to a dimension in the projection. The projection is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to (i.e., uncorrelated with) the preceding components. Each principal component (eigenvector) is associated with an eigenvalue, which corresponds to the amount of the variance explained by that component.\n",
    "\n",
    "Dimensionality reduction is a one-way transformation that induces a loss of information. We can try to minimize the loss of information while retaining the benefits of dimensionality reduction by trying to find the number of principal components needed to effectively represent the original dataset. This number can often be determined by the \"elbow\" or \"knee\" point, which is considered to be the natural break between the useful principal components (or dimensions) and residual noise. We can find the elbow point by computing PCA on our dataset and observing the number of principal components after which the amount of variance explained displays a natural break or drop-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "norm_dist_pca = pca.fit_transform(norm_dist)  #Transform the data into this dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,3))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.scatter(norm_dist[:,0], norm_dist[:,1], color='r')\n",
    "ax.axis('equal') # equal scaling on both axis;\n",
    "ax.set_title('Original', size=24)\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.scatter(norm_dist_pca[:,0], [1 for x in norm_dist_pca[:,0]], color='g')\n",
    "ax.set_title('First Component', size=32)\n",
    "ax.set_yticklabels([])\n",
    "#ax.set_ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much of the datasets variation is explained by the first component?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explained_variance_pct = float(pca.explained_variance_ratio_) * 100\n",
    "print \"{:.1f}% of the data is explained by the first principal component\"\"\".format(explained_variance_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the transformed dataset using the first two principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How much variance is explained using the first two components?\n",
    "- Does the explained variance number make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,3))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.scatter(norm_dist[:,0], norm_dist[:,1], color='r')\n",
    "ax.axis('equal') # equal scaling on both axis;\n",
    "ax.set_title('Original', size=24)\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "#Fill this part in with the transformed data\n",
    "ax.axis('equal') # equal scaling on both axis;\n",
    "ax.set_title('Transformed', size=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Multidimensional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits = datasets.load_digits(n_class=6) # load the dataset with 6 classes (digits 0 through 5)\n",
    "X = pd.DataFrame(digits.data) # explanatory (or independent or feature) variables\n",
    "y = pd.Series(digits.target) # target (or dependent or class) variable\n",
    "n_samples, n_features = X.shape # the number of rows (samples) and columns (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this dataset look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_img_per_row = 20 # number of digits per row\n",
    "img = np.zeros((10*n_img_per_row, 10*n_img_per_row)) # generate a new 200x200 array filled with zeros\n",
    "for i in range(n_img_per_row):\n",
    "    ix = 10 * i + 1\n",
    "    for j in range(n_img_per_row):\n",
    "        iy = 10 * j + 1\n",
    "        img[ix:ix+8, iy:iy+8] = X.ix[i*n_img_per_row + j].reshape((8, 8)) # set each 8x8 area of the img to the values of each row (reshaped from 1x64 to 8x8)\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=250) # define a figure, with size (width and height) and resolution\n",
    "#axes(frameon = 0) # remove the frame/border from the axes\n",
    "plt.imshow(img, cmap=plt.cm.binary) # show the image using a binary color map\n",
    "plt.xticks([]) # no x ticks\n",
    "plt.yticks([]) # no y ticks\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generally, PCA requires centering the data (i.e., subtracting the mean from each data point for each feature), because otherwise the first component may not truly describe the largest direction of variation in the data, but rather the mean of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pca centers the data with unit variance - therefore:\n",
    "np.round(norm_dist_pca.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# global centering\n",
    "X_centered = X - X.mean()\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "for number in range(1,40):\n",
    "    subplot_index = number * 2 - 1\n",
    "    ax = fig.add_subplot(5, 16, subplot_index)\n",
    "    ax.imshow(X.ix[number,:].reshape(8,8))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax = fig.add_subplot(5, 16, subplot_index + 1)\n",
    "    ax.imshow(X_centered.ix[number,:].reshape(8,8))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance explained by number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=64)\n",
    "X_pca = pca.fit_transform(X_centered)\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.bar([x for x in range(len(pca.explained_variance_ratio_))],pca.explained_variance_ratio_)\n",
    "#ax.plot([x for x in range(len(pca.explained_variance_ratio_))],np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "ax.set_title(\"Explained variance\", size=32)\n",
    "ax.set_ylabel(\"Percent explained\", size=24)\n",
    "ax.set_xlabel(\"Principal Component\", size=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is a one-way transformation that induces a loss of information. We can try to minimize the loss of information while retaining the benefits of dimensionality reduction by trying to find the number of principal components needed to effectively represent the original dataset. This number can often be determined by the \"elbow\" or \"knee\" point, which is considered to be the natural break between the useful principal components (or dimensions) and residual noise. We can find the elbow point by computing PCA on our dataset and observing the number of principal components after which the amount of variance explained displays a natural break or drop-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets look at the first 2 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_embedding(X, title=None):\n",
    "    # min-max normalization\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure(figsize=(10, 6), dpi=250)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.axis('off')\n",
    "    ax.patch.set_visible(False)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(digits.target[i]), color=plt.cm.Set1(y[i] / 10.), fontdict={'weight': 'bold', 'size': 12})\n",
    "\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.ylim([-0.1,1.1])\n",
    "    plt.xlim([-0.1,1.1])\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title, fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#It's easy to represent 2 elements in a plot\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_centered)\n",
    "\n",
    "print \"{:.2f}% variance explained by the first {} components\".format((np.sum(pca.explained_variance_ratio_) * 100), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random 2 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random 2D projection using a random unitary matrix\n",
    "\n",
    "#print(\"Computing random projection\"),\n",
    "rp = random_projection.SparseRandomProjection(n_components=2, random_state=42)\n",
    "X_projected = rp.fit_transform(X)\n",
    "#print(\"done.\")\n",
    "\n",
    "# Plot random projection result\n",
    "plot_embedding(X_projected, \"Random Projection of the Digits Dataset\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 2 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_embedding(X_pca, \"2 components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA has no information about the classes, but provides insight into the distribution of different numbers in the parameter space\n",
    "- 0 and 4 tend to be more distinct then 1, 2, 5\n",
    "- Does this make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA is performed using linear combinations of the original features using a truncated Singular Value Decomposition of the matrix X so as to project the data onto a base of the top singular vectors. If the number of retained components is 2 or 3, PCA can be used to visualize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Perform PCA with the first two components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Useful plotting function (lifted from Alessandro)\n",
    "from itertools import cycle\n",
    "\n",
    "def plot_PCA_2D(data, target, target_names):\n",
    "    colors = cycle('rgbcmykw')\n",
    "    target_ids = range(len(target_names))\n",
    "    plt.figure()\n",
    "    for i, c, label in zip(target_ids, colors, target_names):\n",
    "        plt.scatter(data[target == i, 0], data[target == i, 1],\n",
    "                   c=c, label=label)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_PCA_2D(, target=iris.target, target_names=iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#What can you observe about this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If you finish try running the above with a randomized PCA\n",
    "from sklearn.decomposition import RandomizedPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.RandomizedPCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
